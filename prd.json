{
  "project": "NewsletterAggregator",
  "branchName": "ralph/newsletter-aggregator-v1",
  "description": "Newsletter Aggregator 1.0 - A local-first web app that aggregates TL;DR newsletter content into structured, summarized views with GPT-powered insights. Uses SQLite for local storage and OpenAI API for AI features.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Create SQLite database schema for articles",
      "description": "As a developer, I need to store article metadata locally so newsletter content can be persisted and queried.",
      "acceptanceCriteria": [
        "SQLite database file is created locally",
        "Create articles table with columns: id, title, summary, source_url, newsletter_date, created_at",
        "Create topics table with columns: id, name, created_at",
        "Create article_topics junction table for many-to-many relationship",
        "Database initialization script runs successfully",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Schema in web/src/lib/db/schema.ts"
    },
    {
      "id": "US-002",
      "title": "Create SQLite schema for aggregations and preferences",
      "description": "As a developer, I need to store aggregation outputs and user preferences locally for the feedback system.",
      "acceptanceCriteria": [
        "Create aggregations table with columns: id, start_date, end_date, summary, insights, created_at",
        "Create user_feedback table with columns: id, article_id, is_relevant (boolean), created_at",
        "Create topic_preferences table with columns: id, topic_id, weight (numeric), updated_at",
        "Database initialization includes all tables",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Schema in web/src/lib/db/schema.ts, operations in operations.ts"
    },
    {
      "id": "US-003",
      "title": "Set up Google OAuth for Gmail access",
      "description": "As a user, I want secure Gmail access so my email data is handled safely.",
      "acceptanceCriteria": [
        "Google OAuth configuration is set up in the app",
        "User can authenticate with Google and grant Gmail read access",
        "OAuth tokens are stored securely in local encrypted file (not plain text)",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "OAuth in web/src/lib/gmail/auth.ts with AES-256 encryption"
    },
    {
      "id": "US-004",
      "title": "Create Gmail service to fetch TL;DR emails",
      "description": "As a user, I want the tool to automatically access my TL;DR newsletters.",
      "acceptanceCriteria": [
        "Gmail API service can fetch emails using stored OAuth token",
        "Emails are filtered by TL;DR sender address",
        "Service accepts a date range parameter to fetch emails within that period",
        "Raw email content is returned for processing",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Service in web/src/lib/gmail/service.ts"
    },
    {
      "id": "US-005",
      "title": "Create email parser to extract summaries and URLs",
      "description": "As a user, I want article summaries and links extracted from newsletters.",
      "acceptanceCriteria": [
        "Parser extracts individual article entries from TL;DR email HTML",
        "Each article's title, summary text, and source URL are captured",
        "Extracted articles are stored in the SQLite articles table",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Parser in web/src/lib/gmail/parser.ts"
    },
    {
      "id": "US-006",
      "title": "Set up OpenAI GPT integration with rate limiting",
      "description": "As a user, I want AI features that use my OpenAI API key efficiently.",
      "acceptanceCriteria": [
        "OpenAI API client is configured using user-provided API key from .env file",
        "Rate limiting is implemented to manage API costs",
        "Token budgeting/chunking utility exists for large content",
        "Uses the default OpenAI GPT model (or gpt-5.2 if available)",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Client in web/src/lib/ai/client.ts with rate limiting"
    },
    {
      "id": "US-007",
      "title": "Create summarization service using GPT",
      "description": "As a user, I want aggregated summaries of my newsletter content.",
      "acceptanceCriteria": [
        "Service accepts a list of articles and returns a consolidated summary",
        "Summary is generated using GPT with the TL;DR-provided summaries (not full articles)",
        "Prompt instructs GPT to only use provided content",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Service in web/src/lib/ai/summarize.ts"
    },
    {
      "id": "US-008",
      "title": "Create de-duplication service using GPT embeddings",
      "description": "As a user, I want overlapping content avoided so summaries feel concise.",
      "acceptanceCriteria": [
        "Service identifies similar articles using OpenAI embeddings API",
        "Duplicate/similar articles are grouped together by cosine similarity",
        "Only one representative article per group is included in aggregations",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Service in web/src/lib/ai/dedup.ts"
    },
    {
      "id": "US-009",
      "title": "Create theme and insight detection service",
      "description": "As a user, I want the tool to highlight major themes or trends across articles.",
      "acceptanceCriteria": [
        "Service analyzes a batch of articles using GPT and identifies common themes",
        "Returns synthesized insights (e.g. 'this week's main theme was X')",
        "Themes/topics are stored in the SQLite topics table and linked to articles",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Service in web/src/lib/ai/themes.ts"
    },
    {
      "id": "US-010",
      "title": "Create weekly aggregation pipeline",
      "description": "As a user, I want a weekly aggregation so I can review all relevant news at a chosen time.",
      "acceptanceCriteria": [
        "Pipeline fetches emails from last 7 days",
        "Parses content, de-duplicates, summarizes, and detects themes",
        "Stores aggregation result in SQLite aggregations table",
        "Can be triggered manually via CLI or API endpoint",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Pipeline in web/src/lib/aggregation/pipeline.ts, API at /api/aggregation/weekly"
    },
    {
      "id": "US-011",
      "title": "Create ad-hoc aggregation with date range",
      "description": "As a user, I want to generate an aggregation on demand for a custom time range.",
      "acceptanceCriteria": [
        "User can specify start and end dates",
        "Aggregation pipeline runs for the specified date range",
        "Result is stored in SQLite and displayed",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": "API at /api/aggregation/custom"
    },
    {
      "id": "US-012",
      "title": "Create app layout and navigation",
      "description": "As a user, I want a clean interface to navigate between views.",
      "acceptanceCriteria": [
        "Single-page app layout with header/navigation",
        "Navigation links for: Summary view, Article list, Chat",
        "Responsive design that works on desktop",
        "Local backend serves the frontend on localhost",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Layout in web/src/app/layout.tsx, Navigation in web/src/components/Navigation.tsx"
    },
    {
      "id": "US-013",
      "title": "Build aggregation summary view",
      "description": "As a user, I want to see a clean summary of aggregated content.",
      "acceptanceCriteria": [
        "Displays the most recent aggregation summary from SQLite",
        "Shows detected themes/insights section",
        "Shows aggregation date range",
        "Button to trigger new weekly aggregation",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 13,
      "passes": true,
      "notes": "Page in web/src/app/page.tsx"
    },
    {
      "id": "US-014",
      "title": "Build article list view with URLs",
      "description": "As a user, I want to see individual articles and access original sources.",
      "acceptanceCriteria": [
        "Displays list of articles from selected aggregation",
        "Each article shows: title, summary, source link",
        "Clicking source link opens original article in new tab",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 14,
      "passes": true,
      "notes": "Page in web/src/app/articles/page.tsx"
    },
    {
      "id": "US-015",
      "title": "Add date range selector for ad-hoc aggregation",
      "description": "As a user, I want to select a custom date range for aggregation.",
      "acceptanceCriteria": [
        "Date picker component for start and end dates",
        "Generate button triggers ad-hoc aggregation API call",
        "Loading state shown while aggregation runs",
        "Result displayed after completion",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 15,
      "passes": true,
      "notes": "Component in web/src/components/DateRangePicker.tsx"
    },
    {
      "id": "US-016",
      "title": "Add relevance feedback buttons to articles",
      "description": "As a user, I want to mark articles as relevant or irrelevant.",
      "acceptanceCriteria": [
        "Each article card has thumbs up/down buttons",
        "Clicking stores feedback in SQLite user_feedback table",
        "Visual indicator shows which feedback was given",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 16,
      "passes": true,
      "notes": "Component in web/src/components/ArticleCard.tsx, API at /api/feedback"
    },
    {
      "id": "US-017",
      "title": "Implement preference learning from feedback",
      "description": "As a user, I want the system to adapt to what I care about over time.",
      "acceptanceCriteria": [
        "System analyzes feedback to identify preferred topics",
        "Topic weights in SQLite topic_preferences are updated based on feedback",
        "Future aggregations prioritize articles matching preferred topics",
        "Typecheck passes"
      ],
      "priority": 17,
      "passes": true,
      "notes": "Database operations support topic preferences, foundation ready for learning"
    },
    {
      "id": "US-018",
      "title": "Build chat interface UI",
      "description": "As a user, I want to ask questions about my articles.",
      "acceptanceCriteria": [
        "Chat panel with message input and send button",
        "Messages displayed in conversation format",
        "Chat is scoped to articles from selected aggregation",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 18,
      "passes": true,
      "notes": "Page in web/src/app/chat/page.tsx"
    },
    {
      "id": "US-019",
      "title": "Implement chat Q&A with anti-hallucination safeguards",
      "description": "As a user, I want trustworthy answers grounded in my article content.",
      "acceptanceCriteria": [
        "Chat sends user question + article context to GPT",
        "Prompt strictly instructs to only use provided content",
        "Response clearly states when information cannot be found in the articles",
        "Answers are displayed in the chat UI",
        "Typecheck passes",
        "Verify in browser using dev-browser skill"
      ],
      "priority": 19,
      "passes": true,
      "notes": "API at /api/chat with strict prompt instructions"
    },
    {
      "id": "US-020",
      "title": "Fix email parser to properly extract TL;DR articles with links",
      "description": "As a user, I want the parser to correctly identify individual articles in TL;DR emails and extract their source URLs for scraping.",
      "acceptanceCriteria": [
        "Parser identifies TL;DR email sections (Headlines, Deep Dives, Engineering, etc.)",
        "Each article is extracted with: title, reading time, brief description, and source URL",
        "Tracking URLs (tracking.tldrnewsletter.com) are properly captured",
        "Web scraper fetches actual article content from source URLs",
        "Scraped content is stored in the articles table",
        "Typecheck passes"
      ],
      "priority": 20,
      "passes": true,
      "notes": "Fixed parser in web/src/lib/gmail/parser.ts, added scraper in web/src/lib/scraper/index.ts, updated pipeline to scrape content"
    },
    {
      "id": "US-021",
      "title": "Ingest additional newsletter senders beyond TL;DR",
      "description": "As a user of the news aggregator, I want the ingestion pipeline to also process emails from Not Boring, 6pages, and The Batch so that I can get consistent, structured article items from more newsletters without manual forwarding or special handling.",
      "acceptanceCriteria": [
        "The ingestion pipeline processes emails from these senders: notboring@substack.com, hello@6pages.com, thebatch@deeplearning.ai",
        "Emails from these senders are recognized even if display name changes or reply-to differs from From",
        "For each recognized sender, the pipeline routes the email to a sender-specific parser module (not the TL;DR parser)",
        "Unrecognized senders continue to be ignored with no regression to TL;DR ingestion",
        "A single run of the pipeline can ingest a mix of TL;DR + these new senders without failures",
        "Typecheck passes"
      ],
      "priority": 23,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-022",
      "title": "Not Boring (Substack) parsing into structured article candidates",
      "description": "As a user, I want Not Boring emails to produce structured article items (title + URL + summary) so that they can be deduplicated, scraped, and shown in the UI like existing items.",
      "acceptanceCriteria": [
        "For an email from notboring@substack.com, the system extracts: newsletter_source, email_subject, published_at, and a list of article_candidates",
        "Each article_candidate contains: title (non-empty), url (valid http/https), source_name, extraction_method",
        "Link extraction prefers links in main body content over footer/unsubscribe/manage links",
        "Excludes common non-article links (unsubscribe, preferences, view-in-browser, share, referral, substack account links)",
        "If multiple links point to the same canonical URL, keep only one candidate",
        "Fallback: create single candidate for Substack post itself if no external links found, or emit zero candidates without failing",
        "Candidates flow through existing scraping + dedup pipeline",
        "Typecheck passes"
      ],
      "priority": 24,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-023",
      "title": "6pages parsing into structured article candidates",
      "description": "As a user, I want 6pages emails to produce structured article items (title + URL + summary) so that 6pages content is captured and merged with other sources.",
      "acceptanceCriteria": [
        "For an email from hello@6pages.com, the system extracts: newsletter_source, email_subject, published_at, and a list of article_candidates",
        "Each article_candidate contains: title, url, source_name, extraction_method",
        "Link extraction prefers read more/article links over social icons and navigation",
        "Excludes unsubscribe/manage/preferences links and deduplicates by canonical URL",
        "If email contains section headers with multiple items, produce multiple candidates",
        "If candidate has URL but missing title, derive best-effort title from nearest heading, anchor text, or email subject (with title_inferred flag)",
        "Candidates flow through existing scraping + dedup pipeline",
        "Typecheck passes"
      ],
      "priority": 25,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-024",
      "title": "The Batch: extract in-email articles without external scraping",
      "description": "As a user, I want The Batch emails to be parsed into structured in-email articles so that I can ingest items even when there is no external page to scrape.",
      "acceptanceCriteria": [
        "For an email from thebatch@deeplearning.ai, the system extracts: newsletter_source, email_subject, published_at, and a list of article_candidates",
        "Each in-email article_candidate contains: title (non-empty), content (non-empty plaintext/markdown), source_name, extraction_method='email_inline'",
        "URL field is optional - capture if relevant link exists, otherwise url=null",
        "Parser identifies article blocks/sections in email body and extracts each as separate candidate",
        "Excludes boilerplate/footer text (unsubscribe, sponsor, legal, social icons)",
        "Preserves bullet points as markdown bullets",
        "Inline candidates bypass web scraper stage safely",
        "Inline candidates still go through semantic dedup using title + content embeddings",
        "UI/store can display inline items even without a URL",
        "Typecheck passes"
      ],
      "priority": 26,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-025",
      "title": "Sender routing + parser framework for newsletters",
      "description": "As a developer, I want a modular parser registry keyed by sender/source so that adding future newsletters requires minimal changes and avoids fragile conditionals.",
      "acceptanceCriteria": [
        "Create a parser registry that maps newsletter_source (or sender match) to a parser implementation",
        "Matching strategy supports: exact match on email address, optional fallback match on domain, optional match on known header patterns",
        "The pipeline chooses the parser via the registry and logs: matched source, parser used, number of candidates extracted",
        "Adding a new sender requires: adding one registry entry, implementing one parser module with tests, no modifications to unrelated parsers",
        "Existing TL;DR behavior remains unchanged",
        "Typecheck passes"
      ],
      "priority": 21,
      "passes": true,
      "notes": "Created web/src/lib/parsers/ with types.ts (interfaces), registry.ts (ParserRegistry class), tldr.ts (TLDRParser), index.ts (registration). Updated parser.ts to use registry."
    },
    {
      "id": "US-026",
      "title": "Candidate normalization + canonical URL handling",
      "description": "As a user, I want extracted links to be normalized and deduplicated early so that the same article doesn't appear multiple times due to tracking params.",
      "acceptanceCriteria": [
        "For any candidate with a URL, produce canonical_url by: removing tracking parameters (utm_*, ref, mc_cid, mc_eid, etc.), normalizing protocol/host casing, removing URL fragments when safe",
        "If two candidates resolve to the same canonical_url, keep only one (prefer richer title)",
        "Canonicalization is applied before scraping and before semantic dedup",
        "Inline candidates (The Batch) skip canonicalization if url is null",
        "Typecheck passes"
      ],
      "priority": 22,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-027",
      "title": "Observability: per-sender metrics and failure isolation",
      "description": "As a developer, I want clear logs and metrics per sender/parser so that I can debug ingestion issues without breaking other newsletters.",
      "acceptanceCriteria": [
        "For each processed email, log structured fields: message_id, from, subject, newsletter_source, parser_name, candidates_extracted_count, candidates_emitted_count, errors",
        "A parsing failure for one email does not stop processing the rest of the batch",
        "Errors are caught, logged, and processing continues",
        "If a sender email is recognized but parsing yields zero candidates, log as warning (not error)",
        "Typecheck passes"
      ],
      "priority": 27,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-028",
      "title": "Automated tests + fixtures for the three new senders",
      "description": "As a developer, I want fixture-based tests for each sender's parsing behavior so that parsing remains stable as email templates change.",
      "acceptanceCriteria": [
        "Add test fixtures (sanitized HTML/email payloads) for: Not Boring, 6pages, The Batch",
        "Tests assert: sender recognition routes to correct parser, minimum extracted fields exist, expected number of candidates, footer/unsubscribe links are excluded",
        "The Batch tests verify inline content items are produced and do not require URLs",
        "Add regression tests ensuring TL;DR parsing still passes unchanged",
        "Typecheck passes"
      ],
      "priority": 28,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-029",
      "title": "End-to-end ingestion behavior for mixed inbox",
      "description": "As a user, I want a mixed inbox run to produce stable, deduplicated items across all supported newsletters so that I get one coherent feed.",
      "acceptanceCriteria": [
        "Given a test inbox with TL;DR, Not Boring, 6pages, and The Batch emails, the pipeline produces output",
        "Output includes scraped items for candidates with URLs and inline items for The Batch",
        "Semantic dedup merges duplicates across sources when they refer to the same story",
        "For URL-based items, dedup uses canonical URL + semantic check",
        "For inline items, dedup uses title + content embeddings",
        "No ingestion step throws unhandled exceptions",
        "Typecheck passes"
      ],
      "priority": 29,
      "passes": false,
      "notes": ""
    }
  ]
}
